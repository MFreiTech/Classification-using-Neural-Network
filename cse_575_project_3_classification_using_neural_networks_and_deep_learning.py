# -*- coding: utf-8 -*-
"""CSE 575 Project 3 Classification Using Neural Networks and Deep Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rM7yNNYFwNrRHrMnEc5GbBQYoJGLMf0g
"""

# Commented out IPython magic to ensure Python compatibility.
#importing required libraries 

#importing Pandas
import pandas as pd
#importing Matplotlib
import matplotlib.pyplot as plt
# %matplotlib inline
#importing Numpy
import numpy as np
#importing rcParams to set size of the plot
from pylab import rcParams
#impoting Random to randomly plot images of few digits
import random
#impoting warnings module to ignore the warnings if any during execution
import warnings
warnings.filterwarnings('ignore')
#imporing keras
import keras
#importing the MNIST dataset
from keras.datasets import mnist
#Sequential allows to build a model layer by layer
from keras.models import Sequential 
#Dropout is a way of cutting too much association among features by dropping the weights (edges) at a probability. 
# Flatten layers are used when you got a multidimensional output and you want to make it linear to pass it onto a Dense layer.
from keras.layers import Dense, Dropout, Flatten 
# Mandatory Conv2D parameter is the numbers of filters that convolutional layers will learn from.
from keras.layers import Conv2D, MaxPooling2D
#Keras does not handle itself low-level operations such as tensor products, convolutions and so on. 
# Instead, it relies on a specialized, well-optimized tensor manipulation library to do so, serving as the "backend engine" of Keras.
#Google Colab uses Tensorflow as default backend hence for this project Tensorflow backend is used
from keras import backend as K

# The batch size is a number of samples processed before the model is updated
batch_size = 128
#The number of different categories present in the data
num_classes = 10
# The number of epochs is the number of complete passes through the training dataset.
epochs = 12
#appending each pass in a list
epoch_list=[]
for i in range (1,epochs+1):
  epoch_list.append(i)

# input image dimensions
img_rows, img_cols = 28, 28

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

#plots images of few digits randomly
for i in range(9):
  plt.subplot(3,3,i+1)
  plt.tight_layout()
  plt.imshow(x_train[i],cmap='gray', interpolation='none')
  plt.title("Digit: {}".format(y_train[i]))
  plt.xticks([])
  plt.yticks([])

if K.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

"""**Defining the classifier**


> The first argument is **Number of feature maps in the first convolution layer**

> The second argument is the **Number of feature maps in the second convolution layer**

> The third argument is the **Size of the kernel in the first convolution layer**

> The fourth argument is the **Size of the kernel in the second convolution layer**
"""

def CNN(feature_maps_layer_1, feature_maps_layer_2,kernel_size_layer_1,kernel_size_layer_2):
  model = Sequential()
  #adding the first convolution layer with activation function RELU
  model.add(Conv2D(feature_maps_layer_1, kernel_size=kernel_size_layer_1,
                  activation='relu',
                  input_shape=input_shape))
  # 2D Max pooling block represents a max pooling operation
  model.add(MaxPooling2D(pool_size=(2, 2)))
  #adding the second convolution layer with activation function RELU
  model.add(Conv2D(feature_maps_layer_2,kernel_size_layer_2, activation='relu'))
  # 2D Max pooling block represents a max pooling operation
  model.add(MaxPooling2D(pool_size=(2, 2)))
  #Used to convert a multidimensional Tensor into a sigle 1-D tensor
  model.add(Flatten())
  # In dense layer each input is connected to every output by a weight
  #This dense layer will produce a output array of shape (*,120) and uses RELU as activation function
  model.add(Dense(120, activation='relu'))
  #This dense layer will produce a output array of shape (*,84) and uses RELU as activation function
  model.add(Dense(84, activation='relu'))
  #This dense layer will produce a output array of shape (*,10) and uses Softmax as activation function
  model.add(Dense(num_classes, activation='softmax'))

  # https://keras.io/optimizers/ 
  model.compile(loss=keras.losses.categorical_crossentropy,
                optimizer=keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=None, decay=0.0),
                metrics=['accuracy'])
  #Training the user model on the training dataeset 
  user_model = model.fit(x_train, y_train,
            batch_size=batch_size,
            epochs=epochs,
            verbose=1,
            validation_data=(x_test, y_test))
  score = model.evaluate(x_test, y_test, verbose=0)
  print 'Test loss:', score[0] 
  print 'Test accuracy:', score[1]
  metrics = pd.DataFrame({"train_acc":user_model.history['acc'], "train_loss":user_model.history['loss'], "test_acc":user_model.history['val_acc'],"test_loss":user_model.history['val_loss']})
  rcParams['figure.figsize'] = 20, 15
  ax=plt.subplot(111)
  ax.plot(epoch_list,metrics['train_loss'], label='Train Dataset',linestyle='-', marker='o')
  ax.plot(epoch_list,metrics['test_loss'], label='Test Dataset',linestyle='-', marker='o')
  plt.title('Training Error / Testing Error vs Epochs',fontsize=25)
  plt.xlabel('Epochs', fontsize=16)
  plt.ylabel('Error', fontsize=16)
  rng = np.arange(1,13,1)
  ax.set_xticks(rng)
  ax.legend()
  plt.grid()
  plt.show()

"""**Baseline Model**

> This uses a 3x3 kernel for both the first and second convolution layer. The number of feature maps in the baseline code is 6 for the first and 16 for the second convolution layer.
"""

CNN(6,16,(3,3),(3,3))

"""**Modified Model - 1:**

> The kernel size in both the convolution layers is changed to 5x5
"""

CNN(6,16,(5,5),(5,5))

"""**Modified Model - 2:**


> The number of feature maps are changed in both the convolution layers. The number of feature maps in the first convolution layer is 32 and the kernel size is 3x3. The number of feature maps in the second convolution layer is 64 and the kernel size is 3x3.
"""

CNN(32,64,(3,3),(3,3))

"""**Modified Model - 3**

> The number of feature maps in the first convolution layer is 32 and the kernel size is 5x5. The number of feature maps in the second convolution layer is 64 and the kernel size is 5x5.
"""

CNN(32,64,(5,5),(5,5))